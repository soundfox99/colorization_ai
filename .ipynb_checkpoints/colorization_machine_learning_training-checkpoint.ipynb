{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafb597e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Folders\n",
    "train_bw_folder = \"./training_data/filtered_data/training/Input_2\"\n",
    "train_color_input_folder = \"./training_data/filtered_data/training/Input_1\"\n",
    "train_color_output_folder = \"./training_data/filtered_data/training/Output\"\n",
    "\n",
    "# List files\n",
    "train_bw_files = sorted(os.listdir(train_bw_folder))\n",
    "train_color_input_files = sorted(os.listdir(train_color_input_folder))\n",
    "train_color_output_files = sorted(os.listdir(train_color_output_folder))\n",
    "\n",
    "# Ensure filenames match\n",
    "assert train_bw_files == train_color_input_files == train_color_output_files, \"Training filenames do not match!\"\n",
    "\n",
    "# Folders\n",
    "val_bw_folder = \"./training_data/filtered_data/validation/Input_2\"\n",
    "val_color_input_folder = \"./training_data/filtered_data/validation/Input_1\"\n",
    "val_color_output_folder = \"./training_data/filtered_data/validation/Output\"\n",
    "\n",
    "# List files\n",
    "val_bw_files = sorted(os.listdir(val_bw_folder))\n",
    "val_color_input_files = sorted(os.listdir(val_color_input_folder))\n",
    "val_color_output_files = sorted(os.listdir(val_color_output_folder))\n",
    "\n",
    "# Ensure filenames match\n",
    "assert val_bw_files == val_color_input_files == val_color_output_files, \"Validation filenames do not match!\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba61736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch of images from a list of filenames and folder\n",
    "def load_batch_from_folder(folder, files, start_idx, batch_size, color=True):\n",
    "    images = []\n",
    "    end_idx = min(start_idx + batch_size, len(files))\n",
    "    \n",
    "    for i in range(start_idx, end_idx):\n",
    "        img_path = os.path.join(folder, files[i])\n",
    "        # print(img_path)\n",
    "        if color:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        else:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
    "        \n",
    "#         cv2.imshow(\"Image\", img)\n",
    "#         # # Wait for a key press and close the window when a key is pressed\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "        img = img / 255.0\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "gpu_memory_limit = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d05bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate generators for training and validation\n",
    "def train_data_generator(batch_size):\n",
    "    \n",
    "    # Folders\n",
    "    train_bw_input_folder = \"./training_data/filtered_data/training/Input_2\"\n",
    "    train_color_input_folder = \"./training_data/filtered_data/training/Input_1\"\n",
    "    train_color_output_folder = \"./training_data/filtered_data/training/Output\"\n",
    "\n",
    "    # List files\n",
    "    train_bw_input_files = sorted(os.listdir(train_bw_input_folder))\n",
    "    train_color_input_files = sorted(os.listdir(train_color_input_folder))\n",
    "    train_color_output_files = sorted(os.listdir(train_color_output_folder))\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_images = len(train_bw_files)\n",
    "    \n",
    "    while True:\n",
    "        for start_idx in range(0, total_images, batch_size):\n",
    "            bw_batch = load_batch_from_folder(train_bw_input_folder, train_bw_input_files, start_idx, batch_size, color=False)\n",
    "            color_input_batch = load_batch_from_folder(train_color_input_folder, train_color_input_files, start_idx, batch_size)\n",
    "            color_output_batch = load_batch_from_folder(train_color_output_folder, train_color_output_files, start_idx, batch_size)\n",
    "            yield ([bw_batch, color_input_batch], color_output_batch)\n",
    "\n",
    "\n",
    "train_generator = train_data_generator(gpu_memory_limit)\n",
    "\n",
    "\n",
    "# import math\n",
    "# print(math.ceil(len(train_bw_files) / gpu_memory_limit))\n",
    "# for i in range(math.ceil(len(train_bw_files) / gpu_memory_limit)):\n",
    "#     next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33c4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate generators for training and validation\n",
    "def val_data_generator(batch_size):\n",
    "    \n",
    "    # Folders\n",
    "    val_bw_input_folder = \"./training_data/filtered_data/validation/Input_2\"\n",
    "    val_color_input_folder = \"./training_data/filtered_data/validation/Input_1\"\n",
    "    val_color_output_folder = \"./training_data/filtered_data/validation/Output\"\n",
    "\n",
    "    # List files\n",
    "    val_bw_input_files = sorted(os.listdir(val_bw_input_folder))\n",
    "    val_color_input_files = sorted(os.listdir(val_color_input_folder))\n",
    "    val_color_output_files = sorted(os.listdir(val_color_output_folder))\n",
    "    \n",
    "    total_images = len(val_bw_input_files)\n",
    "    \n",
    "    while True:\n",
    "        for start_idx in range(0, total_images, batch_size):\n",
    "            try:\n",
    "                bw_batch = load_batch_from_folder(val_bw_input_folder, val_bw_input_files, start_idx, batch_size, color=False)\n",
    "                color_input_batch = load_batch_from_folder(val_color_input_folder, val_color_input_files, start_idx, batch_size)\n",
    "                color_output_batch = load_batch_from_folder(val_color_output_folder, val_color_output_files, start_idx, batch_size)\n",
    "            except:\n",
    "                continue\n",
    "            yield ([bw_batch, color_input_batch], color_output_batch)\n",
    "\n",
    "\n",
    "val_generator = val_data_generator(gpu_memory_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eae3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# # Data Augmentation\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=20,\n",
    "#     zoom_range=0.15,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.15,\n",
    "#     horizontal_flip=True,\n",
    "#     fill_mode=\"nearest\"\n",
    "# )\n",
    "\n",
    "# # Generator for augmented data\n",
    "# def augmented_data_generator(batch_size):\n",
    "#     base_generator = image_data_generator(batch_size)\n",
    "    \n",
    "#     for bw_batch, color_input_batch, color_output_batch in base_generator:\n",
    "#         # Augment each batch\n",
    "#         # Note: We're using the same seed for both black & white and color input images\n",
    "#         # to ensure they undergo the same transformations.\n",
    "        \n",
    "#         # Augmenting BW images\n",
    "#         bw_gen = datagen.flow(bw_batch, batch_size=batch_size, shuffle=False, seed=42)\n",
    "        \n",
    "#         # Augmenting color input images\n",
    "#         color_input_gen = datagen.flow(color_input_batch, batch_size=batch_size, shuffle=False, seed=42)\n",
    "        \n",
    "#         # Augmenting color output images. Since we need to match outputs with inputs, \n",
    "#         # we're not shuffling and using a consistent seed.\n",
    "#         color_output_gen = datagen.flow(color_output_batch, batch_size=batch_size, shuffle=False, seed=42)\n",
    "        \n",
    "#         yield [next(bw_gen), next(color_input_gen)], next(color_output_gen)\n",
    "\n",
    "# # Example of usage\n",
    "# aug_gen = augmented_data_generator(batch_size)\n",
    "# for (bw_aug_batch, color_input_aug_batch), color_output_aug_batch in aug_gen:\n",
    "#     print(bw_aug_batch.shape, color_input_aug_batch.shape, color_output_aug_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52438ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Concatenate, MaxPooling2D, UpSampling2D\n",
    "\n",
    "def DualInput():\n",
    "    # Black & white input\n",
    "    bw_input = Input(shape=(2048, 1400, 1))\n",
    "    bw_layer1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(bw_input)\n",
    "    bw_layer2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(bw_layer1)\n",
    "    bw_layer3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(bw_layer2)\n",
    "    bw_layer4 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(bw_layer3)\n",
    "    bw_layer5 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(bw_layer4)\n",
    "    bw_layer6 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(bw_layer5)\n",
    "    bw_layer7 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(bw_layer6)\n",
    "    bw_layer8 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(bw_layer7)\n",
    "    \n",
    "    # Colored reference input\n",
    "    color_input = Input(shape=(2048, 1400, 3))\n",
    "    color_layer1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(color_input)\n",
    "    color_layer2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(color_layer1)\n",
    "    color_layer3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(color_layer2)\n",
    "    color_layer4 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(color_layer3)\n",
    "    color_layer5 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(color_layer4)\n",
    "    color_layer6 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(color_layer5)\n",
    "    color_layer7 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(color_layer6)\n",
    "    color_layer8 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(color_layer7)\n",
    "\n",
    "    # Merge inputs\n",
    "    merge_layer1 = Concatenate()([bw_layer8, color_layer8])\n",
    "#     merge_layer = Concatenate()([bw_input, color_input])\n",
    "    \n",
    "    merge_layer2 = (Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))(merge_layer1)\n",
    "    merge_layer3 = (UpSampling2D((2, 2)))(merge_layer2)\n",
    "    merge_layer4 = (Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))(merge_layer3)\n",
    "    merge_layer5 = (Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))(merge_layer4)\n",
    "    merge_layer6 = (UpSampling2D((2, 2)))(merge_layer5)\n",
    "    merge_layer7 = (Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))(merge_layer6)\n",
    "    merge_layer8 = (Conv2D(2, (3, 3), activation=\"relu\", padding=\"same\"))(merge_layer7)\n",
    "    merge_layer9 = (UpSampling2D((2, 2)))(merge_layer8)\n",
    "\n",
    "    # Output layer to produce a 3-channel image\n",
    "    outputs = Conv2D(3, (1, 1), activation='tanh')(merge_layer9)\n",
    "#     outputs = Conv2D(3, (1, 1), activation='tanh')(merge_layer)\n",
    "\n",
    "    return tf.keras.Model(inputs=[bw_input, color_input], outputs=outputs)\n",
    "\n",
    "model = DualInput()\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859f18bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f518854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 2048, 1400,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 2048, 1400,  0           []                               \n",
      "                                 3)]                                                              \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 1024, 700, 6  640         ['input_1[0][0]']                \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 1024, 700, 6  1792        ['input_2[0][0]']                \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 1024, 700, 1  73856       ['conv2d[0][0]']                 \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 1024, 700, 1  73856       ['conv2d_8[0][0]']               \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 512, 350, 12  147584      ['conv2d_1[0][0]']               \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 512, 350, 12  147584      ['conv2d_9[0][0]']               \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 512, 350, 25  295168      ['conv2d_2[0][0]']               \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 512, 350, 25  295168      ['conv2d_10[0][0]']              \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 256, 175, 25  590080      ['conv2d_3[0][0]']               \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 256, 175, 25  590080      ['conv2d_11[0][0]']              \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 256, 175, 51  1180160     ['conv2d_4[0][0]']               \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 256, 175, 51  1180160     ['conv2d_12[0][0]']              \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 256, 175, 51  2359808     ['conv2d_5[0][0]']               \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 256, 175, 51  2359808     ['conv2d_13[0][0]']              \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 256, 175, 25  1179904     ['conv2d_6[0][0]']               \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 256, 175, 25  1179904     ['conv2d_14[0][0]']              \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256, 175, 51  0           ['conv2d_7[0][0]',               \n",
      "                                2)                                'conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 256, 175, 12  589952      ['concatenate[0][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 512, 350, 12  0           ['conv2d_16[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 512, 350, 64  73792       ['up_sampling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 512, 350, 64  36928       ['conv2d_17[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 1024, 700, 6  0          ['conv2d_18[0][0]']              \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 1024, 700, 3  18464       ['up_sampling2d_1[0][0]']        \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 1024, 700, 2  578         ['conv2d_19[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 2048, 1400,   0          ['conv2d_20[0][0]']              \n",
      "                                2)                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 2048, 1400,   9           ['up_sampling2d_2[0][0]']        \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,375,275\n",
      "Trainable params: 12,375,275\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6036a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "steps_per_epoch = math.ceil(len(train_bw_files) / gpu_memory_limit)\n",
    "validation_steps = math.ceil(len(val_bw_files) / gpu_memory_limit)\n",
    " \n",
    "time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "log_dir = \"./logs/fit/\" + time\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "checkpoint_path = f\"./checkpoints/{time}/cp_{time}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                                      patience=2, \n",
    "                                                      verbose=1,\n",
    "                                                     restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a08d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# # 3. Train the model\n",
    "# with tf.device('/GPU:0'):\n",
    "#     history = model.fit(\n",
    "#         train_generator,\n",
    "#         validation_data=val_generator,\n",
    "#         steps_per_epoch=steps_per_epoch,\n",
    "#         validation_steps=validation_steps,\n",
    "#         epochs=5,\n",
    "#         verbose= 1,\n",
    "#         callbacks=[tensorboard_callback, cp_callback, earlystop_callback]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2322a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5:   6%|█▎                      | 1030/18647 [36:09<10:18:25,  2.11s/it, Loss=0.02136201784014702, Val_Loss=0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, steps_per_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m---> 27\u001b[0m     training_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     31\u001b[0m             \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mtrain_data_generator\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, total_images, batch_size):\n\u001b[1;32m---> 20\u001b[0m         bw_batch \u001b[38;5;241m=\u001b[39m \u001b[43mload_batch_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_bw_input_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_bw_input_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m         color_input_batch \u001b[38;5;241m=\u001b[39m load_batch_from_folder(train_color_input_folder, train_color_input_files, start_idx, batch_size)\n\u001b[0;32m     22\u001b[0m         color_output_batch \u001b[38;5;241m=\u001b[39m load_batch_from_folder(train_color_output_folder, train_color_output_files, start_idx, batch_size)\n",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m, in \u001b[0;36mload_batch_from_folder\u001b[1;34m(folder, files, start_idx, batch_size, color)\u001b[0m\n\u001b[0;32m     13\u001b[0m             img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add channel dimension\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#         cv2.imshow(\"Image\", img)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#         # # Wait for a key press and close the window when a key is pressed\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#         cv2.waitKey(0)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#         cv2.destroyAllWindows()\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\n\u001b[0;32m     22\u001b[0m         images\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(images)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, optimizers, metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam()\n",
    "train_acc_metric = metrics.Accuracy()\n",
    "\n",
    "# Custome fit function using tf.GradientTape()\n",
    "EPOCHS = 5\n",
    "ACCUMULATION_BATCHES = 32\n",
    "ACCUMULATION_STEPS = ACCUMULATION_BATCHES // gpu_memory_limit\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    " \n",
    "loss_value_total = 0\n",
    "val_loss = 0\n",
    "# Iterate through multiple epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    processed_data_count = 0\n",
    "    val_loss = 0\n",
    "    loss_value_total = 0\n",
    "    pbar = tqdm(range(1, steps_per_epoch + 1), desc=f\"Epoch: {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    for i in pbar:\n",
    "        training_data = next(train_generator)\n",
    "\n",
    "        with tf.device('/GPU:0'):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                logits = model(training_data[0], training=True)\n",
    "                loss_value = loss_fn(training_data[1], logits)\n",
    "        \n",
    "        \n",
    "        # Backward pass\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Accumulate the gradients\n",
    "        if i % ACCUMULATION_STEPS == 1:\n",
    "            accum_grads = [tf.zeros_like(w) for w in grads]\n",
    "        \n",
    "        accum_grads = [ag + g for ag, g in zip(accum_grads, grads)]\n",
    "\n",
    "        processed_data_count += 1\n",
    "\n",
    "        if (((i % ACCUMULATION_STEPS) == 0) or (i == steps_per_epoch)):\n",
    "            optimizer.apply_gradients(\n",
    "                [(ag / processed_data_count, w) for ag, w in zip(accum_grads, model.trainable_weights)])\n",
    "            accum_grads = [tf.zeros_like(w) for w in grads]\n",
    "            \n",
    "            loss_value_total += loss_value\n",
    "            processed_data_count = 0\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss_value_total / i}\", \"Val_Loss\": f\"{val_loss}\"})\n",
    "            \n",
    "        if (i == steps_per_epoch):\n",
    "            val_loss_sum = 0\n",
    "            for j in range(1, validation_steps + 1):\n",
    "                val_data = next(val_generator)\n",
    "                val_logits = model(val_data[0])\n",
    "                val_loss_sum += loss_fn(val_data[1], val_logits)\n",
    "\n",
    "            val_loss = val_loss_sum / validation_steps\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss_value_total / i}\", \"Val_Loss\": f\"{val_loss}\"})\n",
    "        \n",
    "        # Update the training accuracy metric\n",
    "#         train_acc_metric.update_state(training_data[1], logits)\n",
    "        \n",
    "    # Print the training accuracy at the end of each epoch\n",
    "#     train_acc = train_acc_metric.result()\n",
    "#     print('Training Accuracy:', float(train_acc))\n",
    "#     print(\"Training Loss:\", float(loss_value))\n",
    "#     train_acc_metric.reset_states()\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the last loss and accuracy values from the history object\n",
    "# training_loss = round(history.history['loss'][-1], 4)\n",
    "# training_accuracy = round(history.history['accuracy'][-1], 4)\n",
    "# validation_loss = round(history.history['val_loss'][-1], 4)\n",
    "# validation_accuracy = round(history.history['val_accuracy'][-1], 4)\n",
    "\n",
    "model_save_path = \"./models\"\n",
    "# Save the entire model as a `.keras` zip archive.\n",
    "# model.save(f'{model_save_path}/model-{time}-{training_loss}-{training_accuracy}-{validation_loss}-{validation_accuracy}.keras')\n",
    "\n",
    "model.save(f'{model_save_path}/model-{time}-{(loss_value_total / steps_per_epoch):.6f}-{(val_loss):.6f}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84bdba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e508b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
