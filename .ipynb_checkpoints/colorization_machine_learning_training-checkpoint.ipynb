{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb597e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Folders\n",
    "bw_folder = \"./training_data/filtered_data/Input_2\"\n",
    "color_input_folder = \"./training_data/filtered_data/Input_1\"\n",
    "color_output_folder = \"./training_data/filtered_data/Output\"\n",
    "\n",
    "# List files\n",
    "bw_files = sorted(os.listdir(bw_folder))\n",
    "color_input_files = sorted(os.listdir(color_input_folder))\n",
    "color_output_files = sorted(os.listdir(color_output_folder))\n",
    "\n",
    "# Ensure filenames match\n",
    "assert bw_files == color_input_files == color_output_files, \"Filenames do not match!\"\n",
    "\n",
    "# Split files into training and validation set\n",
    "train_ratio = 0.8\n",
    "bw_train_files, bw_val_files, _, _ = train_test_split(bw_files, bw_files, test_size=1-train_ratio, random_state=42)\n",
    "\n",
    "# Ensure filenames match for training and validation\n",
    "assert set(bw_train_files) & set(bw_val_files) == set(), \"Overlap between training and validation files!\"\n",
    "\n",
    "# Load a batch of images from a list of filenames and folder\n",
    "def load_batch_from_folder(folder, files, start_idx, batch_size, color=True):\n",
    "    images = []\n",
    "    end_idx = min(start_idx + batch_size, len(files))\n",
    "    \n",
    "    for i in range(start_idx, end_idx):\n",
    "        img_path = os.path.join(folder, files[i])\n",
    "        if color:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "            #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
    "        img = img / 255.0\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Separate generators for training and validation\n",
    "def image_data_generator(files, batch_size):\n",
    "    total_images = len(files)\n",
    "    while True:\n",
    "        for start_idx in range(0, total_images, batch_size):\n",
    "            bw_batch = load_batch_from_folder(bw_folder, files, start_idx, batch_size, color=False)\n",
    "            color_input_batch = load_batch_from_folder(color_input_folder, files, start_idx, batch_size)\n",
    "            color_output_batch = load_batch_from_folder(color_output_folder, files, start_idx, batch_size)\n",
    "            yield ([bw_batch, color_input_batch], color_output_batch)\n",
    "\n",
    "# Example of usage\n",
    "batch_size = 2\n",
    "train_generator = image_data_generator(bw_train_files, batch_size)\n",
    "val_generator = image_data_generator(bw_val_files, batch_size)\n",
    "\n",
    "# for input_batch, output_batch in train_generator:\n",
    "#     print('Train:', input_batch[0].shape, input_batch[1].shape, output_batch.shape)\n",
    "\n",
    "# for bw_batch, color_input_batch, color_output_batch in val_generator:\n",
    "#     print('Validation:', bw_batch.shape, color_input_batch.shape, color_output_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# # Data Augmentation\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=20,\n",
    "#     zoom_range=0.15,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.15,\n",
    "#     horizontal_flip=True,\n",
    "#     fill_mode=\"nearest\"\n",
    "# )\n",
    "\n",
    "# # Generator for augmented data\n",
    "# def augmented_data_generator(batch_size):\n",
    "#     base_generator = image_data_generator(batch_size)\n",
    "    \n",
    "#     for bw_batch, color_input_batch, color_output_batch in base_generator:\n",
    "#         # Augment each batch\n",
    "#         # Note: We're using the same seed for both black & white and color input images\n",
    "#         # to ensure they undergo the same transformations.\n",
    "        \n",
    "#         # Augmenting BW images\n",
    "#         bw_gen = datagen.flow(bw_batch, batch_size=batch_size, shuffle=False, seed=42)\n",
    "        \n",
    "#         # Augmenting color input images\n",
    "#         color_input_gen = datagen.flow(color_input_batch, batch_size=batch_size, shuffle=False, seed=42)\n",
    "        \n",
    "#         # Augmenting color output images. Since we need to match outputs with inputs, \n",
    "#         # we're not shuffling and using a consistent seed.\n",
    "#         color_output_gen = datagen.flow(color_output_batch, batch_size=batch_size, shuffle=False, seed=42)\n",
    "        \n",
    "#         yield [next(bw_gen), next(color_input_gen)], next(color_output_gen)\n",
    "\n",
    "# # Example of usage\n",
    "# aug_gen = augmented_data_generator(batch_size)\n",
    "# for (bw_aug_batch, color_input_aug_batch), color_output_aug_batch in aug_gen:\n",
    "#     print(bw_aug_batch.shape, color_input_aug_batch.shape, color_output_aug_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52438ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Concatenate, MaxPooling2D, UpSampling2D\n",
    "\n",
    "def DualInput():\n",
    "    # Black & white input\n",
    "    bw_input = Input(shape=(2048, 1400, 1))\n",
    "    bw_layer1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(bw_input)\n",
    "    bw_layer2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(bw_layer1)\n",
    "    bw_layer3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(bw_layer2)\n",
    "    bw_layer4 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(bw_layer3)\n",
    "    bw_layer5 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(bw_layer4)\n",
    "    bw_layer6 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(bw_layer5)\n",
    "    bw_layer7 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(bw_layer6)\n",
    "    bw_layer8 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(bw_layer7)\n",
    "    \n",
    "    # Colored reference input\n",
    "    color_input = Input(shape=(2048, 1400, 3))\n",
    "    color_layer1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(color_input)\n",
    "    color_layer2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(color_layer1)\n",
    "    color_layer3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(color_layer2)\n",
    "    color_layer4 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(color_layer3)\n",
    "    color_layer5 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(color_layer4)\n",
    "    color_layer6 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(color_layer5)\n",
    "    color_layer7 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(color_layer6)\n",
    "    color_layer8 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(color_layer7)\n",
    "\n",
    "    # Merge inputs\n",
    "    merge_layer1 = Concatenate()([bw_layer8, color_layer8])\n",
    "    \n",
    "    merge_layer2 = (Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))(merge_layer1)\n",
    "    merge_layer3 = (UpSampling2D((2, 2)))(merge_layer2)\n",
    "    merge_layer4 = (Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))(merge_layer3)\n",
    "    merge_layer5 = (Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))(merge_layer4)\n",
    "    merge_layer6 = (UpSampling2D((2, 2)))(merge_layer5)\n",
    "    merge_layer7 = (Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))(merge_layer6)\n",
    "    merge_layer8 = (Conv2D(2, (3, 3), activation=\"relu\", padding=\"same\"))(merge_layer7)\n",
    "    merge_layer9 = (UpSampling2D((2, 2)))(merge_layer8)\n",
    "\n",
    "    # Output layer to produce a 3-channel image\n",
    "    outputs = Conv2D(3, (1, 1), activation='tanh')(merge_layer9)\n",
    "\n",
    "    return tf.keras.Model(inputs=[bw_input, color_input], outputs=outputs)\n",
    "\n",
    "model = DualInput()\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f518854",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6036a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "steps_per_epoch = math.ceil(len(bw_train_files) / batch_size)\n",
    "validation_steps = math.ceil(len(bw_val_files) / batch_size)\n",
    " \n",
    "time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "log_dir = \"./logs/fit/\" + time\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "checkpoint_path = f\"./checkpoints/{time}/cp_{time}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                                      patience=2, \n",
    "                                                      verbose=1,\n",
    "                                                     restore_best_weights=True)\n",
    "\n",
    "# 3. Train the model\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "        epochs=3,\n",
    "        verbose= 1,\n",
    "        callbacks=[tensorboard_callback, cp_callback, earlystop_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the last loss and accuracy values from the history object\n",
    "training_loss = round(history.history['loss'][-1], 2)\n",
    "training_accuracy = round(history.history['accuracy'][-1], 2)\n",
    "validation_loss = round(history.history['val_loss'][-1], 2)\n",
    "validation_accuracy = round(history.history['val_accuracy'][-1], 2)\n",
    "\n",
    "model_save_path = \"./models\"\n",
    "# Save the entire model as a `.keras` zip archive.\n",
    "model.save(f'{model_save_path}/model-{time}-{training_loss}-{training_accuracy}-{validation_loss}-{validation_accuracy}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84bdba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
